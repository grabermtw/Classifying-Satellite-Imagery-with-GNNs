{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Satellite Imagery with GNNs\n",
    "## CIS700\n",
    "\n",
    "### Matt Graber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining the data\n",
    "\n",
    "First, we will obtain the data. We will be attempting to classify the Corrected Reflectance (True Color) Suomi NPP / VIIRS product available from the Global Imagery Browse Services (GIBS) API. To label the data for training and testing, we will be using the Clear Sky Confidence (Day) Suomi NPP / VIRRS product and the Land Water Map (OSM) (from Open Street Map served through GIBS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corrected Reflectance (True Color) Layer example: https://gibs.earthdata.nasa.gov/wms/epsg4326/best/wms.cgi?version=1.3.0&service=WMS&request=GetMap&format=image/png&STYLE=default&bbox=-90,-180,90,180CRS=EPSG:4326&HEIGHT=600&WIDTH=600&TIME=2022-05-01&layers=VIIRS_SNPP_CorrectedReflectance_TrueColor\n",
    "\n",
    "Clear Sky Confidence Layer Example: https://gibs.earthdata.nasa.gov/wms/epsg4326/best/wms.cgi?version=1.3.0&service=WMS&request=GetMap&format=image/png&STYLE=default&bbox=-90,-180,90,180CRS=EPSG:4326&HEIGHT=600&WIDTH=600&TIME=2022-05-01&layers=VIIRS_SNPP_Clear_Sky_Confidence_Day\n",
    "\n",
    "Land Water Map Layer Example: https://gibs.earthdata.nasa.gov/wms/epsg4326/best/wms.cgi?version=1.3.0&service=WMS&request=GetMap&format=image/png&STYLE=default&bbox=-90,-180,90,180CRS=EPSG:4326&HEIGHT=600&WIDTH=600&TIME=2022-05-01&layers=OSM_Land_Water_Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import csv\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "from skimage import io\n",
    "from PIL import Image as plimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading VIIRS_SNPP_CorrectedReflectance_TrueColor images...\n",
      "Downloading images for 2022-05-01...\n",
      "Downloading images for 2022-05-02...\n",
      "Downloading images for 2022-05-03...\n",
      "Downloading images for 2022-05-04...\n",
      "Downloading images for 2022-05-05...\n",
      "Downloading VIIRS_SNPP_Clear_Sky_Confidence_Day images...\n",
      "Downloading images for 2022-05-01...\n",
      "Downloading images for 2022-05-02...\n",
      "Downloading images for 2022-05-03...\n",
      "Downloading images for 2022-05-04...\n",
      "Downloading images for 2022-05-05...\n",
      "Downloading OSM_Land_Water_Map images...\n"
     ]
    }
   ],
   "source": [
    "layers = [\"VIIRS_SNPP_CorrectedReflectance_TrueColor\", \"VIIRS_SNPP_Clear_Sky_Confidence_Day\"]\n",
    "startdate = datetime.date(2022,5,1)\n",
    "enddate = datetime.date(2022,5,6)\n",
    "img_extent_step = 5\n",
    "resolution = 128\n",
    "\n",
    "for layer in layers:\n",
    "    print(\"Downloading {} images...\".format(layer))\n",
    "    layer_outdir = os.path.join(os.getcwd(), \"images\", layer)\n",
    "    currentdate = startdate\n",
    "\n",
    "    while currentdate < enddate:\n",
    "        outdir = os.path.join(layer_outdir, str(currentdate))\n",
    "        \n",
    "         # Create directory if it doesn't exist yet\n",
    "        if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir, exist_ok=True)\n",
    "            \n",
    "        print(\"Downloading images for {}...\".format(currentdate))\n",
    "        for longitude in range(-180,180,img_extent_step):\n",
    "            for latitude in range(-90,90,img_extent_step):\n",
    "                extents = \"{0},{1},{2},{3}\".format(latitude, longitude,\n",
    "                                                latitude + img_extent_step,\n",
    "                                                longitude + img_extent_step)\n",
    "                outfilepath = os.path.join(outdir,'{0}_{1}_{2}.png'.format(layer, currentdate, extents))\n",
    "                # Skip any files that have already been downloaded\n",
    "                # (this enables quick resumption if connection errors are encountered).\n",
    "                # put this in a while-loop in case there's a connection error and\n",
    "                # the download for something needs to be retried\n",
    "                while not os.path.exists(outfilepath) or cv2.imread(outfilepath) is None:\n",
    "                    # Construct image URL.\n",
    "                    url = 'https://gibs.earthdata.nasa.gov/wms/epsg4326/best/wms.cgi?\\\n",
    "version=1.3.0&service=WMS&request=GetMap&\\\n",
    "format=image/png&STYLE=default&bbox={0}&CRS=EPSG:4326&\\\n",
    "HEIGHT={3}&WIDTH={3}&TIME={1}&layers={2}'.format(extents, currentdate, layer, resolution)\n",
    "                    \n",
    "                    # Occasionally we get an error from a momentary dropout of internet connection or something.\n",
    "                    # This try-except should \n",
    "                    try:\n",
    "                        # Request and save image\n",
    "                        img = plimg.fromarray(io.imread(url))\n",
    "                        img.save(outfilepath)\n",
    "                    except:\n",
    "                        print(\"Error encountered, retrying\")\n",
    "                        time.sleep(5)\n",
    "\n",
    "        currentdate += datetime.timedelta(1)\n",
    "\n",
    "# OSM_Land_Water_Map is a static layer, meaning that we don't need to re-download it for every day.\n",
    "layer = \"OSM_Land_Water_Map\"\n",
    "print(\"Downloading {} images...\".format(layer))\n",
    "outdir = os.path.join(os.getcwd(), \"images\", \"{}\".format(layer))\n",
    "\n",
    "# Create directory if it doesn't exist yet\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)\n",
    "\n",
    "for longitude in range(-180,180,img_extent_step):\n",
    "    for latitude in range(-90,90,img_extent_step):\n",
    "        extents = \"{0},{1},{2},{3}\".format(latitude, longitude,\n",
    "                                        latitude + img_extent_step,\n",
    "                                        longitude + img_extent_step)\n",
    "        outfilepath = os.path.join(outdir,'{0}_{1}.png'.format(layer, extents))\n",
    "        # Skip any files that have already been downloaded\n",
    "        # (this enables quick resumption if connection errors are encountered)\n",
    "        while not os.path.exists(outfilepath) or cv2.imread(outfilepath) is None:\n",
    "            # Construct image URL.\n",
    "            url = 'https://gibs.earthdata.nasa.gov/wms/epsg4326/best/wms.cgi?\\\n",
    "version=1.3.0&service=WMS&request=GetMap&\\\n",
    "format=image/png&STYLE=default&bbox={0}&CRS=EPSG:4326&\\\n",
    "HEIGHT={2}&WIDTH={2}&layers={1}'.format(extents, layer, resolution)\n",
    "            # Occasionally we get an error from a momentary dropout of internet connection or something.\n",
    "            # This try-except should \n",
    "            try:\n",
    "                # Request and save image\n",
    "                img = plimg.fromarray(io.imread(url))\n",
    "                img.save(outfilepath)\n",
    "            except:\n",
    "                print(\"Error encountered, retrying\")\n",
    "                time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling the data\n",
    "Next, we will label each Corrected Reflectance image to indicate whether it contains land, water, and/or clouds to use as the training data for the neural networks. We will do this based on the percentages of colors in the corresponding images from Clear Sky Confidence and Land Water Map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling for date 2022-05-01...\n",
      "Labeling for date 2022-05-02...\n",
      "Labeling for date 2022-05-03...\n",
      "Labeling for date 2022-05-04...\n",
      "Labeling for date 2022-05-05...\n",
      "Labeling complete!\n"
     ]
    }
   ],
   "source": [
    "labeled_data_filename = \"labeled_data.csv\"\n",
    "\n",
    "layer_to_label_path = os.path.join(\"images\",\"VIIRS_SNPP_CorrectedReflectance_TrueColor\")\n",
    "clear_sky_layer_path = os.path.join(\"images\", \"VIIRS_SNPP_Clear_Sky_Confidence_Day\")\n",
    "land_water_map_path = os.path.join(\"images\", \"OSM_Land_Water_Map\")\n",
    "lw_filelist = os.listdir(land_water_map_path)\n",
    "\n",
    "resolution = 128\n",
    "pixel_count = resolution ** 2\n",
    "# Exclude any images where 40% or more of the image is \"no data\"\n",
    "nodata_threshold = pixel_count * 0.6\n",
    "\n",
    "# dict for memoization of land water map results, since this is a static layer\n",
    "lw_results = {}\n",
    "\n",
    "with open(labeled_data_filename, 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"filepath\", \"weather\", \"terrain\"])\n",
    "    for date in os.listdir(layer_to_label_path):\n",
    "        print(\"Labeling for date {}...\".format(date))\n",
    "        co_re_datepath = os.path.join(layer_to_label_path, date)\n",
    "        cl_sk_datepath = os.path.join(clear_sky_layer_path, date)\n",
    "        co_re_filelist = os.listdir(co_re_datepath)\n",
    "        cl_sk_filelist = os.listdir(cl_sk_datepath)\n",
    "        for i in range(len(co_re_filelist)):\n",
    "            # these directories should be ordered the same\n",
    "            co_re_imgpath = os.path.join(co_re_datepath, co_re_filelist[i])\n",
    "            cl_sk_imgpath = os.path.join(cl_sk_datepath, cl_sk_filelist[i])\n",
    "            \n",
    "            csv_row = [co_re_imgpath]\n",
    "\n",
    "            # First, check if the corrected reflectance image is in an area of \"no data\"\n",
    "            # i.e. it's all or mostly pure black.\n",
    "            # We want to skip these images.\n",
    "            co_re_img = cv2.imread(co_re_imgpath, 0) # use 0 flag to read grayscale\n",
    "            if cv2.countNonZero(co_re_img) < nodata_threshold:\n",
    "                continue\n",
    "\n",
    "            # Next, check if the image is mostly cloudy or not cloudy.\n",
    "            # In this layer, the reddish color (the higher pixel value) corresponds to clear skies,\n",
    "            # and the whiteish color (the lower pixel value) corresponds to cloudy skies.\n",
    "            cl_sk_img = cv2.imread(cl_sk_imgpath, 0)\n",
    "            \n",
    "            # If there are more dark-colored pixels than light-colored pixels, then it's not cloudy.\n",
    "            if cv2.countNonZero(cv2.inRange(cl_sk_img, 0, 127)) > cv2.countNonZero(cv2.inRange(cl_sk_img, 128, 255)):\n",
    "                # clear skies\n",
    "                csv_row.append(\"clear\")\n",
    "            else:\n",
    "                # cloudy skies\n",
    "                csv_row.append(\"cloudy\")\n",
    "\n",
    "            # Finally, check if the image is mostly land or water.\n",
    "            lw_imgpath = os.path.join(land_water_map_path, lw_filelist[i])\n",
    "            if lw_imgpath in lw_results.keys():\n",
    "                csv_row.append(lw_results[lw_imgpath])\n",
    "            else:\n",
    "                lw_img = cv2.imread(lw_imgpath, 0)\n",
    "                # If there are more light-colored pixels than dark-colored pixels, then its mostly water\n",
    "                if cv2.countNonZero(cv2.inRange(lw_img, 128, 128)) > cv2.countNonZero(cv2.inRange(lw_img, 75, 75)):\n",
    "                    lw_results[lw_imgpath] = \"water\"\n",
    "                else:\n",
    "                    lw_results[lw_imgpath] = \"land\"\n",
    "                csv_row.append(lw_results[lw_imgpath])\n",
    "            writer.writerow(csv_row)\n",
    "\n",
    "print(\"Labeling complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network (non-graph)\n",
    "Now that our data is labeled, we can start by training a baseline non-graph convolutional neural network classifier. This follows the tutorial at https://learnopencv.com/multi-label-image-classification-with-pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/25\n",
      "Epoch: 2/25\n",
      "Epoch: 3/25\n",
      "Epoch: 4/25\n",
      "Epoch: 5/25\n",
      "Epoch: 6/25\n",
      "Epoch: 7/25\n",
      "Epoch: 8/25\n",
      "Epoch: 9/25\n",
      "Epoch: 10/25\n",
      "Epoch: 11/25\n",
      "Epoch: 12/25\n",
      "Epoch: 13/25\n",
      "Epoch: 14/25\n",
      "Epoch: 15/25\n",
      "Epoch: 16/25\n",
      "Epoch: 17/25\n",
      "Epoch: 18/25\n",
      "Epoch: 19/25\n",
      "Epoch: 20/25\n",
      "Epoch: 21/25\n",
      "Epoch: 22/25\n",
      "Epoch: 23/25\n",
      "Epoch: 24/25\n",
      "Epoch: 25/25\n",
      "Total training time: 6437.483986377716 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch as tr\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms, models\n",
    "import random\n",
    "import csv\n",
    "import ast\n",
    "\n",
    "# use a seed to keep the training and testing datasets the same for all models we test\n",
    "random_seed = 44\n",
    "\n",
    "# convert from strings to int representation of the labels\n",
    "def encode(label):\n",
    "    if label in ['clear', 'water']:\n",
    "        return 0\n",
    "    else: # cloudy, land\n",
    "        return 1\n",
    "\n",
    "# Used for obtaining the training/testing data\n",
    "def load_data(filename):\n",
    "    imgs = []\n",
    "    weather = []\n",
    "    terrain = []\n",
    "    with open(filename) as datacsv:\n",
    "        reader = csv.DictReader(datacsv)\n",
    "        for row in reader:\n",
    "            imgs.append(row[\"filepath\"])\n",
    "            weather.append(row[\"weather\"])\n",
    "            terrain.append(row[\"terrain\"])\n",
    "    shufflelist = list(zip(imgs, weather, terrain))\n",
    "    random.Random(random_seed).shuffle(shufflelist)\n",
    "    imgs, weather, terrain = zip(*shufflelist)\n",
    "    imgs, weather, terrain = list(imgs), list(weather), list(terrain)\n",
    "    # split into training and test data (use 60% for training, 40% for testing)\n",
    "    split_size = int(0.6 * len(imgs))\n",
    "    training_data = (imgs[:split_size], weather[:split_size], terrain[:split_size])\n",
    "    testing_data = (imgs[split_size:], weather[split_size:], terrain[split_size:])\n",
    "    return training_data, testing_data\n",
    "\n",
    "class CorrectedReflectanceDataset(tr.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.imgs, self.weather, self.terrain = data\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # take the data sample by its index\n",
    "        img = plimg.open(self.imgs[idx])\n",
    "        # ditch the transparency\n",
    "        img = img.convert('RGB')\n",
    "\n",
    "        # Normalize the image and convert to tensor\n",
    "        # First calculate the mean and standard deviation of pixel values\n",
    "        npimg = np.array(img)\n",
    "        mean = np.mean(npimg, axis=(0,1))\n",
    "        std = np.std(npimg, axis=(0,1))\n",
    "        transform2 = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "        img = transform2(img)\n",
    "        \"\"\"transform2 = transforms.Compose([transforms.ToTensor()])\n",
    "        img = transform2(img)\"\"\"\n",
    "        # return the image and the associated labels\n",
    "        dict_data = {\n",
    "            'img': img,\n",
    "            'labels': {\n",
    "                'weather': encode(self.weather[idx]),\n",
    "                'terrain': encode(self.terrain[idx])\n",
    "            }\n",
    "        }\n",
    "        return dict_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "class MultiOutputModel(nn.Module):\n",
    "    def __init__(self, n_weather_classes, n_terrain_classes):\n",
    "        super().__init__()\n",
    "        self.base_model = models.mobilenet_v2().features  # take the model without classifier\n",
    "        last_channel = models.mobilenet_v2().last_channel # size of the layer before the classifier\n",
    " \n",
    "        # the input for the classifier should be two-dimensional, but we will have\n",
    "        # [<batch_size>, <channels>, <width>, <height>]\n",
    "        # so, let's do the spatial averaging: reduce <width> and <height> to 1\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    " \n",
    "        # create separate classifiers for our outputs\n",
    "        self.weather = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=last_channel, out_features=n_weather_classes)\n",
    "        )\n",
    "        self.terrain = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=last_channel, out_features=n_terrain_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = self.pool(x)\n",
    "    \n",
    "        # reshape from [batch, channels, 1, 1] to [batch, channels] to put it into classifier\n",
    "        x = tr.flatten(x, start_dim=1)\n",
    "    \n",
    "        return {\n",
    "            'weather': self.weather(x),\n",
    "            'terrain': self.terrain(x)\n",
    "        }\n",
    "\n",
    "    def get_loss(self, net_output, ground_truth):\n",
    "        weather_loss = F.cross_entropy(net_output['weather'], ground_truth['weather'])\n",
    "        terrain_loss = F.cross_entropy(net_output['terrain'], ground_truth['terrain'])\n",
    "        \n",
    "        loss = weather_loss + terrain_loss\n",
    "        return loss, {'weather': weather_loss, 'terrain': terrain_loss}\n",
    "\n",
    "def calculate_batch_metrics(predicted, target):\n",
    "    weather_correct_count = 0\n",
    "    terrain_correct_count = 0\n",
    "    \n",
    "    # should all be same length\n",
    "    for i in range(len(predicted['weather'])):\n",
    "        if predicted['weather'][i] == target['weather'][i]:\n",
    "            weather_correct_count += 1\n",
    "        if predicted['terrain'][i] == target['terrain'][i]:\n",
    "            terrain_correct_count += 1\n",
    "    weather_acc = weather_correct_count / len(predicted['weather'])\n",
    "    terrain_acc = terrain_correct_count / len(predicted['terrain'])\n",
    "    return weather_acc, terrain_acc\n",
    "\n",
    "filename = \"labeled_data.csv\"\n",
    "training_data, testing_data = load_data(filename)\n",
    "\n",
    "N_epochs = 25\n",
    "batch_size = 16\n",
    "device = 'cpu'#'cuda:0'\n",
    "outfilename = 'losses_results.csv'\n",
    "\n",
    "\n",
    "training_dataset = CorrectedReflectanceDataset(training_data)\n",
    "train_dataloader = DataLoader(training_dataset, batch_size=batch_size)\n",
    "   \n",
    " \n",
    "model = MultiOutputModel(n_weather_classes=2, n_terrain_classes=2).to(device)\n",
    " \n",
    "optimizer = tr.optim.Adam(model.parameters())\n",
    "\n",
    "# keep track of training time\n",
    "start_time = time.time()\n",
    "with open(outfilename, \"w+\") as f:\n",
    "    csvwriter = csv.writer(f)\n",
    "    csvwriter.writerow([\"epoch\",\"total loss\", \"weather loss\", \"terrain loss\"])\n",
    "    for epoch in range(N_epochs):\n",
    "        print(\"Epoch: {}/{}\".format(epoch+1, N_epochs))\n",
    "        # Set to training mode\n",
    "        model.train()\n",
    "        # Loss within the epoch\n",
    "        train_loss = 0.0\n",
    "        train_losses = {'weather': 0.0, 'terrain': 0.0}\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Clean existing gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            img = batch['img']\n",
    "            target_labels = batch['labels']\n",
    "            \n",
    "            # Forward pass - compute outputs on input data using the model\n",
    "            outputs = model(img.to(device))\n",
    "            target_labels['weather'] = target_labels['weather'].to(device)\n",
    "            target_labels['terrain'] = target_labels['terrain'].to(device)\n",
    "            # Compute loss\n",
    "            loss_train, losses_train = model.get_loss(outputs, target_labels)\n",
    "            #total_loss += loss_train.item()\n",
    "            #batch_accuracy_weather, batch_accuracy_terrain = calculate_batch_metrics(outputs, target_labels)\n",
    "\n",
    "            train_loss += float(loss_train)\n",
    "            train_losses['weather'] += float(losses_train['weather'])\n",
    "            train_losses['terrain'] += float(losses_train['terrain'])\n",
    "\n",
    "            # Backpropagate the gradients\n",
    "            loss_train.backward()\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Weather Accuracy: {:.4f}, Terrain Accuracy: {:.4f}\".format(i, loss_train, batch_accuracy_weather.item(), batch_accuracy_terrain.item()))\n",
    "        csvwriter.writerow([epoch, train_loss, train_losses['weather'], train_losses['terrain']])\n",
    "        f.flush()\n",
    "print(\"Total training time: %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll evaluate the accuracy, precision, recall, and f1 score for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weather results:\n",
      "  Training:\n",
      "    Accuracy: 0.7250231982678627\n",
      "    Precision: 0.7212930202637504\n",
      "    Recall: 0.9900662251655629\n",
      "    F1-score: 0.8345738742091553\n",
      "  Testing:\n",
      "    Accuracy: 0.7214749536178108\n",
      "    Precision: 0.714975845410628\n",
      "    Recall: 0.9929553840992955\n",
      "    F1-score: 0.8313439123718579\n",
      "terrain results:\n",
      "  Training:\n",
      "    Accuracy: 0.3159604082895144\n",
      "    Precision: 0.2912980537236609\n",
      "    Recall: 0.9907002188183808\n",
      "    F1-score: 0.45021752641392176\n",
      "  Testing:\n",
      "    Accuracy: 0.3200371057513915\n",
      "    Precision: 0.297036858588292\n",
      "    Recall: 0.9887730553327987\n",
      "    F1-score: 0.4568358651352353\n",
      "Total runtime: 6735.020060539246\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "testing_dataset = CorrectedReflectanceDataset(testing_data)\n",
    "test_dataloader = DataLoader(testing_dataset, batch_size=batch_size)\n",
    "\n",
    "# put the model into evaluation mode\n",
    "model.eval()\n",
    "\n",
    "for label_type in ['weather', 'terrain']:\n",
    "\n",
    "    print(label_type, \"results:\")\n",
    "    for dataset_type in [\"Training\", \"Testing\"]:\n",
    "        # initialize storage for ground truth and predicted labels\n",
    "        predicted_all = []\n",
    "        gt_all = []\n",
    "        # go over all the images\n",
    "        dataloader = train_dataloader if dataset_type == \"Training\" else test_dataloader\n",
    "        for batch in dataloader:\n",
    "            images = batch[\"img\"]\n",
    "            # we're going to build the confusion matrix for \"weather\" predictions\n",
    "            gts = batch[\"labels\"][label_type]\n",
    "            target_labels = {label_type: gts.to(device)}\n",
    "        \n",
    "            # get the model outputs\n",
    "            output = model(images.to(device))\n",
    "        \n",
    "            # get the most confident prediction for each image\n",
    "            _, predicteds = output[label_type].cpu().max(1)\n",
    "        \n",
    "            predicted_all.extend(\n",
    "                prediction.item() for prediction in predicteds\n",
    "            )\n",
    "            gt_all.extend(\n",
    "                gt.item() for gt in gts\n",
    "            )\n",
    "        \n",
    "        accuracy = accuracy_score(gt_all, predicted_all)\n",
    "        precision = precision_score(gt_all, predicted_all)\n",
    "        recall = recall_score(gt_all, predicted_all)\n",
    "        f1 = f1_score(gt_all, predicted_all)\n",
    "\n",
    "        print(\"  {}:\".format(dataset_type))\n",
    "        print(\"    Accuracy:\", accuracy)\n",
    "        print(\"    Precision:\", precision)\n",
    "        print(\"    Recall:\", recall)\n",
    "        print(\"    F1-score:\", f1)\n",
    "print(\"Total runtime:\", str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc2b5acaa58da8cb98f2f1f9c5a12e7ec9d328c197ca9485c680430a79d86e68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
